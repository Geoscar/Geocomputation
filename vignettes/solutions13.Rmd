---
title: "Chapter 13: Geomarketing"
author: "Robin Lovelace, Jakub Nowosad, Jannes Muenchow"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{geocompr-solutions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

## Prerequisites {-}

The solutions assume the following packages are attached (other packages will be attached when needed):

```{r packages, message=FALSE, warning=FALSE}
library(sf)
library(sp)
library(raster)
library(tidyverse)
library(spData)
```

```{r}
data("census_de", package = "spDataLarge")
input = dplyr::select(census_de, x = x_mp_1km, y = y_mp_1km, pop = Einwohner,
                      women = Frauen_A, mean_age = Alter_D,
                      hh_size = HHGroesse_D)
input_tidy = mutate_all(input, funs(ifelse(. %in% c(-1, -9), NA, .)))
```


# Chapter 13

1) We have used `raster::rasterFromXYZ()` to convert a `input_tidy` into a raster brick. 
Try to achieve the same with the help of the `sp::gridded()` function.

```{r}
input_tidy = st_as_sf(input_tidy, coords = c("x", "y"))
# use the correct projection (see data description)
input_tidy = st_set_crs(input_tidy, 3035)
# convert into an sp-object
input_tidy = as(input_tidy, "Spatial")
gridded(input_tidy) = TRUE
# convert into a raster stack
input_ras = stack(input_tidy)
```

2) Download the csv file containing inhabitant information for a 100 m cell resolution (https://www.zensus2011.de/SharedDocs/Downloads/DE/Pressemitteilung/DemografischeGrunddaten/csv_Bevoelkerung_100m_Gitter.zip?__blob=publicationFile&v=3).
Please note that the unzipped file has a size of 1.23 GB.
To read it into R you can use `readr::read_csv`.
This takes 30 seconds on my machine (16 GB RAM)
`data.table::fread()` might be even faster, and returns an object of class `data.table()`.
Use `as.tibble()` to convert it into a tibble.
Build an inhabitant raster, aggregate it to a cell resolution of 1 km, and compare the difference with the inhabitant raster (`inh`) we have created using class mean values.

```{r, eval=FALSE}
build_census_raster = function(url) {
  download.file(url = url, destfile = file.path(tempdir(), "census.zip"),
                method = "auto", mode = "wb")
  # list the file names
  nms = unzip(file.path(tempdir(), "census.zip"), list = TRUE)
  # unzip only the csv file
  base_name = grep(".csv$", nms$Name, value = TRUE)
  unzip(file.path(tempdir(), "census.zip"), files = base_name, exdir = tempdir())
  # read in the csv file
  input = data.table::fread(file.path(tempdir(), base_name)) %>%
    as.tibble
  input = dplyr::select(input, x = starts_with("x_mp_1"), y = starts_with("y_mp_1"),
                        inh = Einwohner)
  # set -1 and -9 to NA
  input = mutate_all(input, funs(ifelse(. %in% c(-1, -9), NA, .)))
  # convert table into a raster (x and y are cell midpoints)
  coordinates(input) =~ x + y
  # use the correct projection
  proj4string(input) = CRS("+init=epsg:3035")
  gridded(input) = TRUE
  # convert into a raster stack
  raster(input)
}

# download 1km resolution
url = paste0("https://www.zensus2011.de/SharedDocs/Downloads/DE/", 
             "Pressemitteilung/DemografischeGrunddaten/csv_Zensusatlas_", 
             "klassierte_Werte_1km_Gitter.zip?__blob=publicationFile&v=8")
inp_coarse = build_census_raster(url)
# reclassify
rcl = matrix(c(1, 1, 125, 2, 2, 375, 3, 3, 1250, 4, 4, 3000, 5, 5, 6000,
               6, 6, 8000), ncol = 3, byrow = TRUE)
inh_coarse = reclassify(inp_coarse, rcl = rcl, right = NA)

# Download and build 1km inhabitant raster
url = paste0("https://www.zensus2011.de/SharedDocs/Downloads/DE/Pressemitteilung/",
             "DemografischeGrunddaten/csv_Bevoelkerung_100m_Gitter.zip", 
             "?__blob=publicationFile&v=3")
inh_fine = build_census_raster(url)
inh_fine = aggregate(inh_fine, fact = 1000 / res(inp_fine)[1], fun = sum)
inh_fine - inh_coarse  # origin has to be the same
origin(inh_fine) = origin(inh_coarse)
summary(inh_fine - inh_coarse)
plot(inh_fine - inh_coarse)
plot(abs(inh_fine - inh_coarse) > 1000)
cellStats((abs(inh_fine - inh_coarse) > 1000), stat = "sum")
cellStats((abs(inh_fine - inh_coarse) > 5000), stat = "sum")
```

3) Suppose our bike shop predominantly sold electric bikes to older people. 
Change the age raster accordingly, repeat the remaining analyses and compare the changes with our original result.

Basically, we are assuming that especially older people will use an electric bike, therefore, we increase the weights for raster cells where predominantly older people are living.

```{r}
rcl_pop = matrix(c(1, 1, 127, 2, 2, 375, 3, 3, 1250, 
                   4, 4, 3000, 5, 5, 6000, 6, 6, 8000), 
                 ncol = 3, byrow = TRUE)
rcl_women = matrix(c(1, 1, 3, 2, 2, 2, 3, 3, 1, 4, 5, 0), 
                   ncol = 3, byrow = TRUE)
# here we are giving the classes (3 to 5) containing the oldest people the
# highest weight
rcl_age = matrix(c(1, 1, 1, 2, 2, 1, 3, 5, 3),
                 ncol = 3, byrow = TRUE)
rcl_hh = rcl_women
rcl = list(rcl_pop, rcl_women, rcl_age, rcl_hh)

reclass = input_ras
for (i in 1:raster::nlayers(reclass)) {
  reclass[[i]] = reclassify(x = reclass[[i]], rcl = rcl[[i]], right = NA)
}
names(reclass) = names(input_ras)
```

The rest of the analysis follows exactly the code presented in the book. 
Supposing you have already created `input_ras` in the first exercise, the analysis would look something like this:

```{r}
# attach packages
library(leaflet)
# attach data
data("metro_names", "shops", package = "spDataLarge")

# Add metro names to metros sf object
#************************************
metro_names = 
  dplyr::select(metro_names, locality, administrative_area_level_2) %>%
  # replace W端lfrath and umlaut 端
  mutate(locality = ifelse(locality == "W端lfrath",
                           administrative_area_level_2,
                           locality),
         locality = gsub("端", "ue", locality)) %>%
  pull(locality)
pop_agg = aggregate(reclass$pop, fact = 20, fun = sum)
pop_agg = pop_agg[pop_agg > 500000, drop = FALSE] 
polys = pop_agg %>% 
  clump %>%
  rasterToPolygons %>%
  st_as_sf
metros = polys %>%
  group_by(clumps) %>%
  summarize
metros$metro_names = metro_names

# Create shop/poi density raster
#*******************************
shops = st_transform(shops, proj4string(reclass))
# create poi raster
poi = rasterize(x = shops, y = reclass, field = "osm_id", fun = "count")
# construct reclassification matrix
int = classInt::classIntervals(values(poi), n = 4, style = "fisher")
int = round(int$brks)
rcl_poi = matrix(c(int[1], rep(int[-c(1, length(int))], each = 2), 
                   int[length(int)] + 1), ncol = 2, byrow = TRUE)
rcl_poi = cbind(rcl_poi, 0:3)  
# reclassify
poi = reclassify(poi, rcl = rcl_poi, right = NA) 
names(poi) = "poi"
# add poi raster
reclass = addLayer(reclass, poi)
# delete population raster
reclass = dropLayer(reclass, "pop")

# Identify suitable locations
#****************************
# calculate the total score
result = sum(reclass)

# have a look at suitable bike shop locations in Berlin
berlin = metros[metro_names == "Berlin", ]
berlin_raster = raster::crop(result, as(berlin, "Spatial"))
# summary(berlin_raster)
# berlin_raster
berlin_raster = berlin_raster > 9
berlin_raster = berlin_raster == TRUE
berlin_raster[berlin_raster == 0] = NA
# make the plot
leaflet() %>% 
  addTiles() %>%
  addRasterImage(berlin_raster, colors = "darkgreen", opacity = 0.8) %>%
  addLegend("bottomright", colors = c("darkgreen"), 
            labels = c("potential locations"), title = "Legend")
```
